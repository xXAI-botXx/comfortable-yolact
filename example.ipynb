{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXME -> have to get tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLACT\n",
    "Config as Parameter Fork (+ Python 3.12)\n",
    "\n",
    "This file shows, how you can use this fork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Installation\n",
    "\n",
    "1. Install Anaconda\n",
    "2. Open your bash/terminal and navigate to this folder\n",
    "3. \n",
    "    ```python\n",
    "    conda env create -f environment.yml\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nn-lib\n",
    "import sys\n",
    "sys.path.append(\"./yolact\")\n",
    "\n",
    "# not needed here but if is outside from this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Import YOLACT - import what you need\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Yolact\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, yolact_base_config\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNetBackbone\n",
      "File \u001b[0;32m~/src/yolact/yolact.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MovingAverage, make_net\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# This is required for Pytorch 1.0.1 on Windows to initialize Cuda on some driver versions.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# See the bug report here: https://github.com/pytorch/pytorch/issues/17108\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# As of March 10, 2019, Pytorch DataParallel still doesn't support JIT Script Modules\u001b[39;00m\n\u001b[1;32m     25\u001b[0m use_jit \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolact/lib/python3.12/site-packages/torch/cuda/__init__.py:778\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolact/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Image Utils\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch & Torch-Utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "# Import YOLACT - import what you need\n",
    "from yolact import Yolact\n",
    "from data import Config, yolact_base_config\n",
    "from backbone import ResNetBackbone\n",
    "from utils.augmentations import FastBaseTransform\n",
    "from utils.logger import Log\n",
    "from eval import prep_display\n",
    "# from layers.output_utils import postprocess\n",
    "from layers.modules import MultiBoxLoss\n",
    "from utils.functions import MovingAverage\n",
    "from layers.output_utils import postprocess\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = Config({\n",
    "    'tanh':    torch.tanh,\n",
    "    'sigmoid': torch.sigmoid,\n",
    "    'softmax': lambda x: torch.nn.functional.softmax(x, dim=-1),\n",
    "    'relu':    lambda x: torch.nn.functional.relu(x, inplace=True),\n",
    "    'none':    lambda x: x,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = Config({\n",
    "    'name': \"WISDOM\",\n",
    "\n",
    "    # Training images and annotations\n",
    "    'train_images': \"~/data/train/images\",\n",
    "    'train_info':   \"-\",\n",
    "    'train_img_folder_path': \"~/data/train/images/images\", \n",
    "    'train_mask_folder_path': \"~/data/train/images/masks\", \n",
    "\n",
    "    # Validation images and annotations.\n",
    "    'valid_images': \"~/data/test/images\",\n",
    "    'valid_info':   \"-\",\n",
    "    'valid_img_folder_path': \"~/data/test/images/images\", \n",
    "    'valid_mask_folder_path': \"~/data/test/images/masks\", \n",
    "\n",
    "    # Whether or not to load GT. If this is False, eval.py quantitative evaluation won't work.\n",
    "    'has_gt': True,\n",
    "\n",
    "    # A list of names for each of you classes.\n",
    "    'class_names': [\"object\"]*80,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_transform = Config({\n",
    "    'channel_order': 'RGB',\n",
    "    'normalize': True,\n",
    "    'subtract_means': False,\n",
    "    'to_float': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_config = Config({\n",
    "    'name': 'ResNet101',\n",
    "    'path': 'resnet101_reducedfc.pth',\n",
    "    'type': ResNetBackbone,\n",
    "    'args': ([3, 4, 23, 3],),\n",
    "    'transform': resnet_transform,\n",
    "\n",
    "    'selected_layers': list(range(1, 4)),\n",
    "    'pred_scales': [[24], [48], [96], [192], [384]],\n",
    "    'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,\n",
    "\n",
    "    'use_pixel_scales': True,\n",
    "    'preapply_sqrt': False,\n",
    "    'use_square_anchors': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_config = Config({\n",
    "    'num_features': 256,\n",
    "    'interpolation_mode': 'bilinear',\n",
    "    'num_downsample': 2,\n",
    "    'use_conv_downsample': True,\n",
    "    'pad': True,\n",
    "    'relu_downsample_layers': False,\n",
    "    'relu_pred_layers': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yolact_config = Config({\n",
    "\n",
    "    'name': \"YOLACT Example\",\n",
    "\n",
    "    ############\n",
    "    ### Data ###\n",
    "    ############\n",
    "\n",
    "    'dataset': data_config,\n",
    "    'num_classes': len(data_config.class_names) + 1, # This should include the background class\n",
    "    'max_size': 550,\n",
    "\n",
    "    #    in SSD\n",
    "    # Randomize hue, vibrance, etc.\n",
    "    'augment_photometric_distort': True,\n",
    "    # Have a chance to scale down the image and pad (to emulate smaller detections)\n",
    "    'augment_expand': True,\n",
    "    # Potentialy sample a random crop from the image and put it in a random place\n",
    "    'augment_random_sample_crop': True,\n",
    "    # Mirror the image with a probability of 1/2\n",
    "    'augment_random_mirror': True,\n",
    "    # Flip the image vertically with a probability of 1/2\n",
    "    'augment_random_flip': False,\n",
    "    # With uniform probability, rotate the image [0,90,180,270] degrees\n",
    "    'augment_random_rot90': False,\n",
    "\n",
    "\n",
    "\n",
    "    ########################\n",
    "    ### Training Details ###\n",
    "    ########################\n",
    "\n",
    "    'max_iter': 20*(50000//5), \n",
    "    'lr': 1e-3,\n",
    "    'momentum': 0.9,\n",
    "    'freeze_bn': False,\n",
    "    'fpn': fpn_config,\n",
    "\n",
    "    'decay': 5e-4,\n",
    "    'gamma': 0.1,\n",
    "    'lr_steps': (280000, 600000, 700000, 750000),\n",
    "    'lr_warmup_init': 1e-4,\n",
    "    'lr_warmup_until': 500,\n",
    "\n",
    "    #    backbone\n",
    "    'backbone': backbone_config,\n",
    "\n",
    "    #     scale loss\n",
    "    'conf_alpha': 1,\n",
    "    'bbox_alpha': 1.5,\n",
    "    'mask_alpha': 0.4 / 256 * 140 * 140, \n",
    "\n",
    "    'use_semantic_segmentation_loss': True,\n",
    "    'semantic_segmentation_alpha': 1,\n",
    "\n",
    "    'use_mask_scoring': False,\n",
    "    'mask_scoring_alpha': 1,\n",
    "\n",
    "    'use_focal_loss': False,\n",
    "    'focal_loss_alpha': 0.25,\n",
    "    'focal_loss_gamma': 2,\n",
    "    'focal_loss_init_pi': 0.01,\n",
    "\n",
    "\n",
    "\n",
    "    #################\n",
    "    ### Detection ###\n",
    "    #################\n",
    "\n",
    "    'max_num_detections': 100,\n",
    "    'eval_mask_branch': True,   # False,\n",
    "    \n",
    "    'nms_top_k': 200,\n",
    "    'nms_conf_thresh': 0.005,\n",
    "    'nms_thresh': 0.5,\n",
    "\n",
    "    'mask_type': 1,\n",
    "    'mask_size': 6.125,\n",
    "    'masks_to_train': 100,\n",
    "    'mask_proto_src': 0,\n",
    "    'mask_proto_net': [(256, 3, {'padding': 1})] * 3 + [(None, -2, {}), (256, 3, {'padding': 1})] + [(32, 1, {})],\n",
    "    'mask_proto_bias': False,\n",
    "    'mask_proto_prototype_activation': activation_func.relu,\n",
    "    'mask_proto_mask_activation': activation_func.sigmoid,\n",
    "    'mask_proto_coeff_activation': activation_func.tanh,\n",
    "    'mask_proto_crop': True,\n",
    "    'mask_proto_crop_expand': 0,\n",
    "    'mask_proto_loss': None,\n",
    "    'mask_proto_binarize_downsampled_gt': True,\n",
    "    'mask_proto_normalize_mask_loss_by_sqrt_area': False,\n",
    "    'mask_proto_reweight_mask_loss': False,\n",
    "    'mask_proto_grid_file': 'data/grid.npy',\n",
    "    'mask_proto_use_grid':  False,\n",
    "    'mask_proto_coeff_gate': False,\n",
    "    'mask_proto_prototypes_as_features': False,\n",
    "    'mask_proto_prototypes_as_features_no_grad': False,\n",
    "    'mask_proto_remove_empty_masks': False,\n",
    "    'mask_proto_reweight_coeff': 1,\n",
    "    'mask_proto_coeff_diversity_loss': False,\n",
    "    'mask_proto_coeff_diversity_alpha': 1,\n",
    "    'mask_proto_normalize_emulate_roi_pooling': True,\n",
    "    'mask_proto_double_loss': False,\n",
    "    'mask_proto_double_loss_alpha': 1,\n",
    "    'mask_proto_split_prototypes_by_head': False,\n",
    "    'mask_proto_crop_with_pred_box': False,\n",
    "    'mask_proto_debug': False,\n",
    "\n",
    "    'discard_box_width': 4 / 550,\n",
    "    'discard_box_height': 4 / 550,\n",
    "\n",
    "    'share_prediction_module': True,\n",
    "    'ohem_use_most_confident': False,\n",
    "\n",
    "    'use_class_balanced_conf': False,\n",
    "\n",
    "    'use_sigmoid_focal_loss': False,\n",
    "\n",
    "    'use_objectness_score': False,\n",
    "\n",
    "    'use_class_existence_loss': False,\n",
    "    'class_existence_alpha': 1,\n",
    "\n",
    "    'use_change_matching': False,\n",
    "\n",
    "    'extra_head_net': [(256, 3, {'padding': 1})],\n",
    "\n",
    "    'head_layer_params': {'kernel_size': 3, 'padding': 1},\n",
    "\n",
    "    'extra_layers': (0, 0, 0),\n",
    "\n",
    "    'positive_iou_threshold': 0.5,\n",
    "    'negative_iou_threshold': 0.4,\n",
    "\n",
    "    'ohem_negpos_ratio': 3,\n",
    "\n",
    "    'crowd_iou_threshold': 0.7,\n",
    "    \n",
    "    'force_cpu_nms': True,\n",
    "\n",
    "    'use_coeff_nms': False,\n",
    "\n",
    "    'use_instance_coeff': False,\n",
    "    'num_instance_coeffs': 64,\n",
    "\n",
    "    'train_masks': True,\n",
    "    'train_boxes': True,\n",
    "    'use_gt_bboxes': False,\n",
    "\n",
    "    'preserve_aspect_ratio': False,\n",
    "\n",
    "    'use_prediction_module': False,\n",
    "\n",
    "    'use_yolo_regressors': False,\n",
    "    \n",
    "    'use_prediction_matching': False,\n",
    "\n",
    "    'delayed_settings': [],\n",
    "\n",
    "    'no_jit': False,\n",
    "\n",
    "    'mask_dim': None,\n",
    "\n",
    "    'use_maskiou': True, \n",
    "    \n",
    "    'maskiou_net': [(8, 3, {'stride': 2}), (16, 3, {'stride': 2}), (32, 3, {'stride': 2}), (64, 3, {'stride': 2}), (128, 3, {'stride': 2})],\n",
    "\n",
    "    'discard_mask_area': 5*5, # -1,\n",
    "\n",
    "    'maskiou_alpha': 25, # 6.125,\n",
    "    'rescore_mask': True,\n",
    "    'rescore_bbox': False,\n",
    "    'maskious_to_train': -1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = train_yolact_config.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(configuration=cfg, should_compute_validation_map=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"./res/test.jpg\"\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = torch.from_numpy(image).cuda().float()\n",
    "\n",
    "prepared_image = FastBaseTransform()(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = Yolact()\n",
    "    model.eval()\n",
    "    model.load_weights(\"./weights/example.pth\")\n",
    "    model.cuda()\n",
    "\n",
    "    # create and visualize the results\n",
    "    preds = model(prepared_image)\n",
    "\n",
    "    img_numpy = prep_display(preds, image, None, None, undo_transform=False,\n",
    "                                configuration=cfg)\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "    classes, scores, boxes, masks = postprocess(preds, w, h, batch_idx=0, interpolation_mode='bilinear',\n",
    "                                                visualize_lincomb=False, crop_masks=True, score_threshold=0,\n",
    "                                                configuration=cfg)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img_numpy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
